{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amir-mjafari/classification_Oxford_102_flowers/blob/main/fellow_ai_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srZr5E7fPxJR",
        "outputId": "13d73a73-f612-46c1-d784-40e8569a9403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfmrIoPIXh4F"
      },
      "outputs": [],
      "source": [
        "# UnZIP files\n",
        "# file name: oxford-102-flowers.zip\n",
        "\n",
        "!unzip gdrive/My\\ Drive/oxford-102-flowers.zip > /flowers\n",
        "\n",
        "# from so on, we use flowers directory to acces the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y-pLxOHag9w",
        "outputId": "458983f9-1173-4f59-9cec-c0e7085b05bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     path  label\n",
            "0     jpg/image_03860.jpg     16\n",
            "1     jpg/image_06092.jpg     13\n",
            "2     jpg/image_02400.jpg     42\n",
            "3     jpg/image_02852.jpg     55\n",
            "4     jpg/image_07710.jpg     96\n",
            "...                   ...    ...\n",
            "1015  jpg/image_02944.jpg     59\n",
            "1016  jpg/image_07434.jpg     93\n",
            "1017  jpg/image_02684.jpg     57\n",
            "1018  jpg/image_01639.jpg     81\n",
            "1019  jpg/image_03165.jpg     10\n",
            "\n",
            "[1020 rows x 2 columns]\n",
            "                     path  label\n",
            "0     jpg/image_04467.jpg     89\n",
            "1     jpg/image_07129.jpg     44\n",
            "2     jpg/image_05166.jpg      4\n",
            "3     jpg/image_07002.jpg     34\n",
            "4     jpg/image_02007.jpg     79\n",
            "...                   ...    ...\n",
            "1015  jpg/image_08182.jpg     61\n",
            "1016  jpg/image_07029.jpg     38\n",
            "1017  jpg/image_05956.jpg     67\n",
            "1018  jpg/image_06051.jpg     13\n",
            "1019  jpg/image_07724.jpg     96\n",
            "\n",
            "[1020 rows x 2 columns]\n",
            "                     path  label\n",
            "0     jpg/image_06977.jpg     34\n",
            "1     jpg/image_00800.jpg     80\n",
            "2     jpg/image_05038.jpg     58\n",
            "3     jpg/image_06759.jpg      0\n",
            "4     jpg/image_01133.jpg     45\n",
            "...                   ...    ...\n",
            "6144  jpg/image_08168.jpg     61\n",
            "6145  jpg/image_07487.jpg     94\n",
            "6146  jpg/image_04432.jpg     89\n",
            "6147  jpg/image_02532.jpg     75\n",
            "6148  jpg/image_00178.jpg     76\n",
            "\n",
            "[6149 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# loading paths and labels\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('oxford-102-flowers/train.txt', header = None, delimiter = \"\\s+\", index_col = False )\n",
        "train_data.columns = [\"path\", \"label\"]\n",
        "\n",
        "valid_data = pd.read_csv('oxford-102-flowers/valid.txt', header = None, delimiter = \"\\s+\", index_col = False )\n",
        "valid_data.columns = [\"path\", \"label\"]\n",
        "\n",
        "test_data = pd.read_csv('oxford-102-flowers/test.txt', header = None, delimiter = \"\\s+\", index_col = False )\n",
        "test_data.columns = [\"path\", \"label\"]\n",
        "\n",
        "print(train_data)\n",
        "print(valid_data)\n",
        "print(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viaqYr3UoEhZ"
      },
      "outputs": [],
      "source": [
        "# loading TRAIN/TEST/VALID images + TRAIN/TEST/VALID labels\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_data(x):\n",
        "\n",
        "  #x is a DataFrame passed into the function\n",
        "\n",
        "  datasets = 'oxford-102-flowers'\n",
        "      \n",
        "  images = []\n",
        "      \n",
        "  for img_path in x['path']:\n",
        "          \n",
        "      IMAGE_SIZE = (224,224)\n",
        "\n",
        "\n",
        "      image_path = datasets +'/' + str(img_path)\n",
        "\n",
        "      image = cv2.imread(image_path)\n",
        "      # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Test!\n",
        "      image = cv2.resize(image, IMAGE_SIZE) \n",
        "\n",
        "      images.append(image)\n",
        "          \n",
        "  images = np.array(images, dtype = 'float32')\n",
        "  labels = np.array(x['label'], dtype = 'int32')\n",
        "\n",
        "  return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbBMOtN61TBP"
      },
      "outputs": [],
      "source": [
        "# loading data with load_data() function by passing in the \n",
        "# test/train/valid DataFrames including 'path' and 'label'\n",
        "\n",
        "train_images, train_labels = load_data(train_data)\n",
        "valid_images, valid_labels = load_data(valid_data)\n",
        "test_images, test_labels = load_data(test_data)\n",
        "# print(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGnXPzEXYivd",
        "outputId": "fd5d09ae-a4b1-4784-a75b-04ad9a7fbab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[ 24.  74.  76.]\n",
            "   [ 22.  72.  74.]\n",
            "   [ 27.  76.  78.]\n",
            "   ...\n",
            "   [ 57. 102.  99.]\n",
            "   [ 57. 102.  99.]\n",
            "   [ 57. 102.  99.]]\n",
            "\n",
            "  [[ 27.  78.  80.]\n",
            "   [ 25.  76.  78.]\n",
            "   [ 28.  77.  79.]\n",
            "   ...\n",
            "   [ 58. 103. 100.]\n",
            "   [ 58. 103. 100.]\n",
            "   [ 57. 102.  99.]]\n",
            "\n",
            "  [[ 27.  78.  80.]\n",
            "   [ 27.  78.  80.]\n",
            "   [ 25.  74.  76.]\n",
            "   ...\n",
            "   [ 57. 104. 101.]\n",
            "   [ 57. 104. 101.]\n",
            "   [ 55. 102.  99.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 79.  94. 110.]\n",
            "   [ 77.  92. 108.]\n",
            "   [ 75.  90. 106.]\n",
            "   ...\n",
            "   [104. 117. 133.]\n",
            "   [ 99. 112. 128.]\n",
            "   [ 97. 110. 126.]]\n",
            "\n",
            "  [[ 78.  93. 110.]\n",
            "   [ 77.  93. 108.]\n",
            "   [ 75.  92. 108.]\n",
            "   ...\n",
            "   [101. 115. 133.]\n",
            "   [ 97. 111. 129.]\n",
            "   [ 95. 109. 127.]]\n",
            "\n",
            "  [[ 72.  89. 104.]\n",
            "   [ 72.  87. 104.]\n",
            "   [ 71.  87. 103.]\n",
            "   ...\n",
            "   [101. 115. 133.]\n",
            "   [ 97. 111. 129.]\n",
            "   [ 95. 109. 127.]]]\n",
            "\n",
            "\n",
            " [[[  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]]\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]]\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [ 62.  87.  83.]\n",
            "   [ 77.  98.  95.]\n",
            "   [ 28.  48.  44.]]\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [ 62.  83.  80.]\n",
            "   [ 44.  50.  49.]\n",
            "   [  5.   7.   7.]]\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [  1.  14.  11.]\n",
            "   [  0.   0.   0.]\n",
            "   [  5.   3.   3.]]]\n",
            "\n",
            "\n",
            " [[[ 29. 106. 161.]\n",
            "   [ 29. 111. 147.]\n",
            "   [ 22. 113. 121.]\n",
            "   ...\n",
            "   [239. 223. 216.]\n",
            "   [249. 226. 218.]\n",
            "   [251. 226. 216.]]\n",
            "\n",
            "  [[ 22. 104. 146.]\n",
            "   [ 21. 108. 134.]\n",
            "   [ 16. 110. 108.]\n",
            "   ...\n",
            "   [248. 227. 219.]\n",
            "   [253. 228. 218.]\n",
            "   [253. 228. 218.]]\n",
            "\n",
            "  [[  9.  96. 115.]\n",
            "   [ 11. 103. 109.]\n",
            "   [  9. 108.  91.]\n",
            "   ...\n",
            "   [252. 226. 219.]\n",
            "   [252. 227. 217.]\n",
            "   [252. 227. 217.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[163. 180. 167.]\n",
            "   [162. 183. 168.]\n",
            "   [ 92. 119.  99.]\n",
            "   ...\n",
            "   [255. 247. 234.]\n",
            "   [255. 246. 236.]\n",
            "   [255. 246. 236.]]\n",
            "\n",
            "  [[133. 147. 133.]\n",
            "   [ 53.  71.  55.]\n",
            "   [  8.  36.  16.]\n",
            "   ...\n",
            "   [255. 247. 234.]\n",
            "   [255. 246. 236.]\n",
            "   [255. 246. 236.]]\n",
            "\n",
            "  [[ 22.  53.  35.]\n",
            "   [  9.  50.  28.]\n",
            "   [ 21.  72.  44.]\n",
            "   ...\n",
            "   [255. 247. 234.]\n",
            "   [255. 245. 235.]\n",
            "   [255. 245. 235.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  1.   6.   5.]\n",
            "   [  1.   3.   3.]\n",
            "   [  4.   4.   4.]\n",
            "   ...\n",
            "   [ 19.  23. 105.]\n",
            "   [ 16.  21.  97.]\n",
            "   [ 12.  15.  89.]]\n",
            "\n",
            "  [[  2.   7.   6.]\n",
            "   [  0.   5.   4.]\n",
            "   [  4.   4.   4.]\n",
            "   ...\n",
            "   [ 21.  24. 107.]\n",
            "   [ 17.  22. 100.]\n",
            "   [ 14.  17.  91.]]\n",
            "\n",
            "  [[  5.  11.  10.]\n",
            "   [  4.   9.   9.]\n",
            "   [  5.   7.   7.]\n",
            "   ...\n",
            "   [ 22.  25. 110.]\n",
            "   [ 19.  23. 102.]\n",
            "   [ 15.  18.  93.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  7.  12.  11.]\n",
            "   [  7.  12.  11.]\n",
            "   [  8.  13.  12.]\n",
            "   ...\n",
            "   [ 10.  25.  21.]\n",
            "   [ 11.  25.  21.]\n",
            "   [ 11.  22.  19.]]\n",
            "\n",
            "  [[  7.  13.  12.]\n",
            "   [  7.  13.  12.]\n",
            "   [  7.  13.  12.]\n",
            "   ...\n",
            "   [ 14.  25.  23.]\n",
            "   [ 12.  22.  20.]\n",
            "   [  9.  22.  20.]]\n",
            "\n",
            "  [[  7.  13.  12.]\n",
            "   [  7.  13.  12.]\n",
            "   [  7.  13.  12.]\n",
            "   ...\n",
            "   [ 14.  22.  21.]\n",
            "   [ 11.  22.  20.]\n",
            "   [  9.  22.  20.]]]\n",
            "\n",
            "\n",
            " [[[ 24.  87.  61.]\n",
            "   [ 31.  90.  65.]\n",
            "   [ 37.  93.  68.]\n",
            "   ...\n",
            "   [  5.   5.   5.]\n",
            "   [  8.   3.   4.]\n",
            "   [  7.   6.   8.]]\n",
            "\n",
            "  [[ 31.  93.  67.]\n",
            "   [ 35.  93.  68.]\n",
            "   [ 41.  97.  71.]\n",
            "   ...\n",
            "   [  1.   6.   5.]\n",
            "   [  2.   6.   5.]\n",
            "   [  6.   8.   9.]]\n",
            "\n",
            "  [[ 36.  94.  69.]\n",
            "   [ 38.  95.  70.]\n",
            "   [ 43.  97.  73.]\n",
            "   ...\n",
            "   [  0.   6.   5.]\n",
            "   [  0.   8.   6.]\n",
            "   [  3.   8.   9.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 45.  65. 113.]\n",
            "   [ 45.  63. 110.]\n",
            "   [ 42.  59. 106.]\n",
            "   ...\n",
            "   [ 85.  95. 113.]\n",
            "   [ 83.  93. 111.]\n",
            "   [ 83.  90. 109.]]\n",
            "\n",
            "  [[ 44.  60. 106.]\n",
            "   [ 39.  58. 102.]\n",
            "   [ 35.  55.  96.]\n",
            "   ...\n",
            "   [ 84.  94. 112.]\n",
            "   [ 83.  93. 111.]\n",
            "   [ 81.  91. 109.]]\n",
            "\n",
            "  [[ 41.  57. 102.]\n",
            "   [ 36.  55.  98.]\n",
            "   [ 32.  52.  93.]\n",
            "   ...\n",
            "   [ 84.  94. 112.]\n",
            "   [ 83.  93. 111.]\n",
            "   [ 81.  91. 109.]]]\n",
            "\n",
            "\n",
            " [[[118. 141. 137.]\n",
            "   [119. 142. 138.]\n",
            "   [120. 142. 138.]\n",
            "   ...\n",
            "   [ 38.  57.  42.]\n",
            "   [ 37.  55.  42.]\n",
            "   [ 37.  53.  43.]]\n",
            "\n",
            "  [[118. 139. 136.]\n",
            "   [118. 139. 136.]\n",
            "   [119. 140. 137.]\n",
            "   ...\n",
            "   [ 35.  54.  39.]\n",
            "   [ 35.  53.  40.]\n",
            "   [ 36.  53.  43.]]\n",
            "\n",
            "  [[116. 137. 134.]\n",
            "   [117. 138. 135.]\n",
            "   [117. 138. 135.]\n",
            "   ...\n",
            "   [ 33.  51.  38.]\n",
            "   [ 31.  49.  36.]\n",
            "   [ 34.  51.  40.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 39.  72.  57.]\n",
            "   [ 37.  68.  53.]\n",
            "   [ 32.  59.  45.]\n",
            "   ...\n",
            "   [ 82. 105.  91.]\n",
            "   [ 69.  92.  78.]\n",
            "   [ 53.  73.  60.]]\n",
            "\n",
            "  [[ 46.  78.  61.]\n",
            "   [ 47.  76.  60.]\n",
            "   [ 42.  68.  54.]\n",
            "   ...\n",
            "   [ 68.  93.  75.]\n",
            "   [ 61.  83.  65.]\n",
            "   [ 50.  68.  52.]]\n",
            "\n",
            "  [[ 51.  80.  64.]\n",
            "   [ 49.  78.  62.]\n",
            "   [ 45.  70.  56.]\n",
            "   ...\n",
            "   [ 56.  83.  64.]\n",
            "   [ 50.  72.  54.]\n",
            "   [ 42.  60.  43.]]]]\n",
            "(6149,)\n"
          ]
        }
      ],
      "source": [
        "print(train_images)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs2bw_kbZMI_"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as K\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_of_class = len(np.unique(test_labels))\n",
        "def prep_data(x, y):\n",
        "\n",
        "    # x: Numpy array of images (n, 224, 224, 3) \n",
        "    # y: Numpy array of labels  \n",
        "\n",
        "\n",
        "    x_p = K.applications.resnet50.preprocess_input(x)\n",
        "    y_p = K.utils.to_categorical(y, num_of_class)\n",
        "\n",
        "    return x_p, y_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBOytEE5ijOP"
      },
      "outputs": [],
      "source": [
        "prep_train_images, prep_train_labels = prep_data(train_images, train_labels)\n",
        "prep_valid_images, prep_valid_labels = prep_data(valid_images, valid_labels)\n",
        "prep_test_images, prep_test_labels = prep_data(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d1T9oF7pzU6",
        "outputId": "f5813b27-8ebe-4499-ca5f-c8ee62397bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[-2.7939003e+01 -4.2778999e+01 -9.9680000e+01]\n",
            "   [-2.9939003e+01 -4.4778999e+01 -1.0168000e+02]\n",
            "   [-2.5939003e+01 -4.0778999e+01 -9.6680000e+01]\n",
            "   ...\n",
            "   [-4.9390030e+00 -1.4778999e+01 -6.6680000e+01]\n",
            "   [-4.9390030e+00 -1.4778999e+01 -6.6680000e+01]\n",
            "   [-4.9390030e+00 -1.4778999e+01 -6.6680000e+01]]\n",
            "\n",
            "  [[-2.3939003e+01 -3.8778999e+01 -9.6680000e+01]\n",
            "   [-2.5939003e+01 -4.0778999e+01 -9.8680000e+01]\n",
            "   [-2.4939003e+01 -3.9778999e+01 -9.5680000e+01]\n",
            "   ...\n",
            "   [-3.9390030e+00 -1.3778999e+01 -6.5680000e+01]\n",
            "   [-3.9390030e+00 -1.3778999e+01 -6.5680000e+01]\n",
            "   [-4.9390030e+00 -1.4778999e+01 -6.6680000e+01]]\n",
            "\n",
            "  [[-2.3939003e+01 -3.8778999e+01 -9.6680000e+01]\n",
            "   [-2.3939003e+01 -3.8778999e+01 -9.6680000e+01]\n",
            "   [-2.7939003e+01 -4.2778999e+01 -9.8680000e+01]\n",
            "   ...\n",
            "   [-2.9390030e+00 -1.2778999e+01 -6.6680000e+01]\n",
            "   [-2.9390030e+00 -1.2778999e+01 -6.6680000e+01]\n",
            "   [-4.9390030e+00 -1.4778999e+01 -6.8680000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 6.0609970e+00 -2.2778999e+01 -4.4680000e+01]\n",
            "   [ 4.0609970e+00 -2.4778999e+01 -4.6680000e+01]\n",
            "   [ 2.0609970e+00 -2.6778999e+01 -4.8680000e+01]\n",
            "   ...\n",
            "   [ 2.9060997e+01  2.2100067e-01 -1.9680000e+01]\n",
            "   [ 2.4060997e+01 -4.7789993e+00 -2.4680000e+01]\n",
            "   [ 2.2060997e+01 -6.7789993e+00 -2.6680000e+01]]\n",
            "\n",
            "  [[ 6.0609970e+00 -2.3778999e+01 -4.5680000e+01]\n",
            "   [ 4.0609970e+00 -2.3778999e+01 -4.6680000e+01]\n",
            "   [ 4.0609970e+00 -2.4778999e+01 -4.8680000e+01]\n",
            "   ...\n",
            "   [ 2.9060997e+01 -1.7789993e+00 -2.2680000e+01]\n",
            "   [ 2.5060997e+01 -5.7789993e+00 -2.6680000e+01]\n",
            "   [ 2.3060997e+01 -7.7789993e+00 -2.8680000e+01]]\n",
            "\n",
            "  [[ 6.0997009e-02 -2.7778999e+01 -5.1680000e+01]\n",
            "   [ 6.0997009e-02 -2.9778999e+01 -5.1680000e+01]\n",
            "   [-9.3900299e-01 -2.9778999e+01 -5.2680000e+01]\n",
            "   ...\n",
            "   [ 2.9060997e+01 -1.7789993e+00 -2.2680000e+01]\n",
            "   [ 2.5060997e+01 -5.7789993e+00 -2.6680000e+01]\n",
            "   [ 2.3060997e+01 -7.7789993e+00 -2.8680000e+01]]]\n",
            "\n",
            "\n",
            " [[[-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   ...\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]]\n",
            "\n",
            "  [[-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   ...\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]]\n",
            "\n",
            "  [[-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   ...\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   ...\n",
            "   [-2.0939003e+01 -2.9778999e+01 -6.1680000e+01]\n",
            "   [-8.9390030e+00 -1.8778999e+01 -4.6680000e+01]\n",
            "   [-5.9939003e+01 -6.8778999e+01 -9.5680000e+01]]\n",
            "\n",
            "  [[-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   ...\n",
            "   [-2.3939003e+01 -3.3778999e+01 -6.1680000e+01]\n",
            "   [-5.4939003e+01 -6.6778999e+01 -7.9680000e+01]\n",
            "   [-9.6939003e+01 -1.0977900e+02 -1.1868000e+02]]\n",
            "\n",
            "  [[-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   ...\n",
            "   [-9.2939003e+01 -1.0277900e+02 -1.2268000e+02]\n",
            "   [-1.0393900e+02 -1.1677900e+02 -1.2368000e+02]\n",
            "   [-1.0093900e+02 -1.1377900e+02 -1.1868000e+02]]]\n",
            "\n",
            "\n",
            " [[[ 5.7060997e+01 -1.0778999e+01 -9.4680000e+01]\n",
            "   [ 4.3060997e+01 -5.7789993e+00 -9.4680000e+01]\n",
            "   [ 1.7060997e+01 -3.7789993e+00 -1.0168000e+02]\n",
            "   ...\n",
            "   [ 1.1206100e+02  1.0622100e+02  1.1532000e+02]\n",
            "   [ 1.1406100e+02  1.0922100e+02  1.2532000e+02]\n",
            "   [ 1.1206100e+02  1.0922100e+02  1.2732000e+02]]\n",
            "\n",
            "  [[ 4.2060997e+01 -1.2778999e+01 -1.0168000e+02]\n",
            "   [ 3.0060997e+01 -8.7789993e+00 -1.0268000e+02]\n",
            "   [ 4.0609970e+00 -6.7789993e+00 -1.0768000e+02]\n",
            "   ...\n",
            "   [ 1.1506100e+02  1.1022100e+02  1.2432000e+02]\n",
            "   [ 1.1406100e+02  1.1122100e+02  1.2932001e+02]\n",
            "   [ 1.1406100e+02  1.1122100e+02  1.2932001e+02]]\n",
            "\n",
            "  [[ 1.1060997e+01 -2.0778999e+01 -1.1468000e+02]\n",
            "   [ 5.0609970e+00 -1.3778999e+01 -1.1268000e+02]\n",
            "   [-1.2939003e+01 -8.7789993e+00 -1.1468000e+02]\n",
            "   ...\n",
            "   [ 1.1506100e+02  1.0922100e+02  1.2832001e+02]\n",
            "   [ 1.1306100e+02  1.1022100e+02  1.2832001e+02]\n",
            "   [ 1.1306100e+02  1.1022100e+02  1.2832001e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 6.3060997e+01  6.3221001e+01  3.9320000e+01]\n",
            "   [ 6.4060997e+01  6.6221001e+01  3.8320000e+01]\n",
            "   [-4.9390030e+00  2.2210007e+00 -3.1680000e+01]\n",
            "   ...\n",
            "   [ 1.3006100e+02  1.3022101e+02  1.3132001e+02]\n",
            "   [ 1.3206100e+02  1.2922101e+02  1.3132001e+02]\n",
            "   [ 1.3206100e+02  1.2922101e+02  1.3132001e+02]]\n",
            "\n",
            "  [[ 2.9060997e+01  3.0221001e+01  9.3199997e+00]\n",
            "   [-4.8939003e+01 -4.5778999e+01 -7.0680000e+01]\n",
            "   [-8.7939003e+01 -8.0778999e+01 -1.1568000e+02]\n",
            "   ...\n",
            "   [ 1.3006100e+02  1.3022101e+02  1.3132001e+02]\n",
            "   [ 1.3206100e+02  1.2922101e+02  1.3132001e+02]\n",
            "   [ 1.3206100e+02  1.2922101e+02  1.3132001e+02]]\n",
            "\n",
            "  [[-6.8939003e+01 -6.3778999e+01 -1.0168000e+02]\n",
            "   [-7.5939003e+01 -6.6778999e+01 -1.1468000e+02]\n",
            "   [-5.9939003e+01 -4.4778999e+01 -1.0268000e+02]\n",
            "   ...\n",
            "   [ 1.3006100e+02  1.3022101e+02  1.3132001e+02]\n",
            "   [ 1.3106100e+02  1.2822101e+02  1.3132001e+02]\n",
            "   [ 1.3106100e+02  1.2822101e+02  1.3132001e+02]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-9.8939003e+01 -1.1077900e+02 -1.2268000e+02]\n",
            "   [-1.0093900e+02 -1.1377900e+02 -1.2268000e+02]\n",
            "   [-9.9939003e+01 -1.1277900e+02 -1.1968000e+02]\n",
            "   ...\n",
            "   [ 1.0609970e+00 -9.3778999e+01 -1.0468000e+02]\n",
            "   [-6.9390030e+00 -9.5778999e+01 -1.0768000e+02]\n",
            "   [-1.4939003e+01 -1.0177900e+02 -1.1168000e+02]]\n",
            "\n",
            "  [[-9.7939003e+01 -1.0977900e+02 -1.2168000e+02]\n",
            "   [-9.9939003e+01 -1.1177900e+02 -1.2368000e+02]\n",
            "   [-9.9939003e+01 -1.1277900e+02 -1.1968000e+02]\n",
            "   ...\n",
            "   [ 3.0609970e+00 -9.2778999e+01 -1.0268000e+02]\n",
            "   [-3.9390030e+00 -9.4778999e+01 -1.0668000e+02]\n",
            "   [-1.2939003e+01 -9.9778999e+01 -1.0968000e+02]]\n",
            "\n",
            "  [[-9.3939003e+01 -1.0577900e+02 -1.1868000e+02]\n",
            "   [-9.4939003e+01 -1.0777900e+02 -1.1968000e+02]\n",
            "   [-9.6939003e+01 -1.0977900e+02 -1.1868000e+02]\n",
            "   ...\n",
            "   [ 6.0609970e+00 -9.1778999e+01 -1.0168000e+02]\n",
            "   [-1.9390030e+00 -9.3778999e+01 -1.0468000e+02]\n",
            "   [-1.0939003e+01 -9.8778999e+01 -1.0868000e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-9.2939003e+01 -1.0477900e+02 -1.1668000e+02]\n",
            "   [-9.2939003e+01 -1.0477900e+02 -1.1668000e+02]\n",
            "   [-9.1939003e+01 -1.0377900e+02 -1.1568000e+02]\n",
            "   ...\n",
            "   [-8.2939003e+01 -9.1778999e+01 -1.1368000e+02]\n",
            "   [-8.2939003e+01 -9.1778999e+01 -1.1268000e+02]\n",
            "   [-8.4939003e+01 -9.4778999e+01 -1.1268000e+02]]\n",
            "\n",
            "  [[-9.1939003e+01 -1.0377900e+02 -1.1668000e+02]\n",
            "   [-9.1939003e+01 -1.0377900e+02 -1.1668000e+02]\n",
            "   [-9.1939003e+01 -1.0377900e+02 -1.1668000e+02]\n",
            "   ...\n",
            "   [-8.0939003e+01 -9.1778999e+01 -1.0968000e+02]\n",
            "   [-8.3939003e+01 -9.4778999e+01 -1.1168000e+02]\n",
            "   [-8.3939003e+01 -9.4778999e+01 -1.1468000e+02]]\n",
            "\n",
            "  [[-9.1939003e+01 -1.0377900e+02 -1.1668000e+02]\n",
            "   [-9.1939003e+01 -1.0377900e+02 -1.1668000e+02]\n",
            "   [-9.1939003e+01 -1.0377900e+02 -1.1668000e+02]\n",
            "   ...\n",
            "   [-8.2939003e+01 -9.4778999e+01 -1.0968000e+02]\n",
            "   [-8.3939003e+01 -9.4778999e+01 -1.1268000e+02]\n",
            "   [-8.3939003e+01 -9.4778999e+01 -1.1468000e+02]]]\n",
            "\n",
            "\n",
            " [[[-4.2939003e+01 -2.9778999e+01 -9.9680000e+01]\n",
            "   [-3.8939003e+01 -2.6778999e+01 -9.2680000e+01]\n",
            "   [-3.5939003e+01 -2.3778999e+01 -8.6680000e+01]\n",
            "   ...\n",
            "   [-9.8939003e+01 -1.1177900e+02 -1.1868000e+02]\n",
            "   [-9.9939003e+01 -1.1377900e+02 -1.1568000e+02]\n",
            "   [-9.5939003e+01 -1.1077900e+02 -1.1668000e+02]]\n",
            "\n",
            "  [[-3.6939003e+01 -2.3778999e+01 -9.2680000e+01]\n",
            "   [-3.5939003e+01 -2.3778999e+01 -8.8680000e+01]\n",
            "   [-3.2939003e+01 -1.9778999e+01 -8.2680000e+01]\n",
            "   ...\n",
            "   [-9.8939003e+01 -1.1077900e+02 -1.2268000e+02]\n",
            "   [-9.8939003e+01 -1.1077900e+02 -1.2168000e+02]\n",
            "   [-9.4939003e+01 -1.0877900e+02 -1.1768000e+02]]\n",
            "\n",
            "  [[-3.4939003e+01 -2.2778999e+01 -8.7680000e+01]\n",
            "   [-3.3939003e+01 -2.1778999e+01 -8.5680000e+01]\n",
            "   [-3.0939003e+01 -1.9778999e+01 -8.0680000e+01]\n",
            "   ...\n",
            "   [-9.8939003e+01 -1.1077900e+02 -1.2368000e+02]\n",
            "   [-9.7939003e+01 -1.0877900e+02 -1.2368000e+02]\n",
            "   [-9.4939003e+01 -1.0877900e+02 -1.2068000e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 9.0609970e+00 -5.1778999e+01 -7.8680000e+01]\n",
            "   [ 6.0609970e+00 -5.3778999e+01 -7.8680000e+01]\n",
            "   [ 2.0609970e+00 -5.7778999e+01 -8.1680000e+01]\n",
            "   ...\n",
            "   [ 9.0609970e+00 -2.1778999e+01 -3.8680000e+01]\n",
            "   [ 7.0609970e+00 -2.3778999e+01 -4.0680000e+01]\n",
            "   [ 5.0609970e+00 -2.6778999e+01 -4.0680000e+01]]\n",
            "\n",
            "  [[ 2.0609970e+00 -5.6778999e+01 -7.9680000e+01]\n",
            "   [-1.9390030e+00 -5.8778999e+01 -8.4680000e+01]\n",
            "   [-7.9390030e+00 -6.1778999e+01 -8.8680000e+01]\n",
            "   ...\n",
            "   [ 8.0609970e+00 -2.2778999e+01 -3.9680000e+01]\n",
            "   [ 7.0609970e+00 -2.3778999e+01 -4.0680000e+01]\n",
            "   [ 5.0609970e+00 -2.5778999e+01 -4.2680000e+01]]\n",
            "\n",
            "  [[-1.9390030e+00 -5.9778999e+01 -8.2680000e+01]\n",
            "   [-5.9390030e+00 -6.1778999e+01 -8.7680000e+01]\n",
            "   [-1.0939003e+01 -6.4778999e+01 -9.1680000e+01]\n",
            "   ...\n",
            "   [ 8.0609970e+00 -2.2778999e+01 -3.9680000e+01]\n",
            "   [ 7.0609970e+00 -2.3778999e+01 -4.0680000e+01]\n",
            "   [ 5.0609970e+00 -2.5778999e+01 -4.2680000e+01]]]\n",
            "\n",
            "\n",
            " [[[ 3.3060997e+01  2.4221001e+01 -5.6800003e+00]\n",
            "   [ 3.4060997e+01  2.5221001e+01 -4.6800003e+00]\n",
            "   [ 3.4060997e+01  2.5221001e+01 -3.6800003e+00]\n",
            "   ...\n",
            "   [-6.1939003e+01 -5.9778999e+01 -8.5680000e+01]\n",
            "   [-6.1939003e+01 -6.1778999e+01 -8.6680000e+01]\n",
            "   [-6.0939003e+01 -6.3778999e+01 -8.6680000e+01]]\n",
            "\n",
            "  [[ 3.2060997e+01  2.2221001e+01 -5.6800003e+00]\n",
            "   [ 3.2060997e+01  2.2221001e+01 -5.6800003e+00]\n",
            "   [ 3.3060997e+01  2.3221001e+01 -4.6800003e+00]\n",
            "   ...\n",
            "   [-6.4939003e+01 -6.2778999e+01 -8.8680000e+01]\n",
            "   [-6.3939003e+01 -6.3778999e+01 -8.8680000e+01]\n",
            "   [-6.0939003e+01 -6.3778999e+01 -8.7680000e+01]]\n",
            "\n",
            "  [[ 3.0060997e+01  2.0221001e+01 -7.6800003e+00]\n",
            "   [ 3.1060997e+01  2.1221001e+01 -6.6800003e+00]\n",
            "   [ 3.1060997e+01  2.1221001e+01 -6.6800003e+00]\n",
            "   ...\n",
            "   [-6.5939003e+01 -6.5778999e+01 -9.0680000e+01]\n",
            "   [-6.7939003e+01 -6.7778999e+01 -9.2680000e+01]\n",
            "   [-6.3939003e+01 -6.5778999e+01 -8.9680000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-4.6939003e+01 -4.4778999e+01 -8.4680000e+01]\n",
            "   [-5.0939003e+01 -4.8778999e+01 -8.6680000e+01]\n",
            "   [-5.8939003e+01 -5.7778999e+01 -9.1680000e+01]\n",
            "   ...\n",
            "   [-1.2939003e+01 -1.1778999e+01 -4.1680000e+01]\n",
            "   [-2.5939003e+01 -2.4778999e+01 -5.4680000e+01]\n",
            "   [-4.3939003e+01 -4.3778999e+01 -7.0680000e+01]]\n",
            "\n",
            "  [[-4.2939003e+01 -3.8778999e+01 -7.7680000e+01]\n",
            "   [-4.3939003e+01 -4.0778999e+01 -7.6680000e+01]\n",
            "   [-4.9939003e+01 -4.8778999e+01 -8.1680000e+01]\n",
            "   ...\n",
            "   [-2.8939003e+01 -2.3778999e+01 -5.5680000e+01]\n",
            "   [-3.8939003e+01 -3.3778999e+01 -6.2680000e+01]\n",
            "   [-5.1939003e+01 -4.8778999e+01 -7.3680000e+01]]\n",
            "\n",
            "  [[-3.9939003e+01 -3.6778999e+01 -7.2680000e+01]\n",
            "   [-4.1939003e+01 -3.8778999e+01 -7.4680000e+01]\n",
            "   [-4.7939003e+01 -4.6778999e+01 -7.8680000e+01]\n",
            "   ...\n",
            "   [-3.9939003e+01 -3.3778999e+01 -6.7680000e+01]\n",
            "   [-4.9939003e+01 -4.4778999e+01 -7.3680000e+01]\n",
            "   [-6.0939003e+01 -5.6778999e+01 -8.1680000e+01]]]]\n"
          ]
        }
      ],
      "source": [
        "print(prep_train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZID__zQct-2j"
      },
      "outputs": [],
      "source": [
        "# Define a convolutional neural network + ResNet50 pretrained implication\n",
        "\n",
        "def create_model(freez_layers):\n",
        "\n",
        "    num_of_class = len(np.unique(test_labels))\n",
        "    f = freez_layers\n",
        "    dimension = prep_train_images.shape[1:]\n",
        "    input_t = K.Input(shape = dimension)\n",
        "    res_model = K.applications.ResNet50(include_top=False, \n",
        "                                        weights='imagenet',\n",
        "                                        input_tensor=input_t)\n",
        "    \n",
        "    # freezing the first 'freez_level' of the ResNet50\n",
        "\n",
        "    for layer in res_model.layers[:f]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    for i, layer in enumerate(res_model.layers):\n",
        "        print(i, layer.name, \"-\", layer.trainable)\n",
        "\n",
        "    model = K.models.Sequential()\n",
        "    model.add(res_model)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(256, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(128, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(64, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(num_of_class, activation='softmax'))\n",
        "\n",
        "\n",
        "    return model   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65648hs8xNP9"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, data_augmentation, opt):\n",
        "\n",
        "  \n",
        "\n",
        "    # Compile the model before using it\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    # create a callback that will save the best model while training\n",
        "    save_best_model = K.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=0)\n",
        "\n",
        "    # train without data augmentation\n",
        "    if not data_augmentation:\n",
        "        print('Not using data augmentation.')\n",
        "        history = model.fit(prep_train_images, prep_train_labels,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(prep_valid_images, prep_valid_labels),\n",
        "                            shuffle=True,\n",
        "                            callbacks=[save_best_model],\n",
        "                            verbose=1)\n",
        "\n",
        "    # train with data augmentation\n",
        "    else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = K.preprocessing.image.ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(prep_train_images)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history = model.fit(datagen.flow(prep_train_images, prep_train_labels, batch_size=batch_size),\n",
        "                            steps_per_epoch=math.ceil(prep_train_images.shape[0]/batch_size),\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(prep_valid_images, prep_valid_labels),\n",
        "                            callbacks=[save_best_model],\n",
        "                            verbose=1)\n",
        "    \n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv_M6Jd44Dzp",
        "outputId": "d7ad02f8-fd6a-4125-a828-b2f6a1c9bdf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5314465485268091404\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCttD3sE4NYY",
        "outputId": "46d5cdd4-07ff-4368-a343-763199ba244f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "0 input_1 - False\n",
            "1 conv1_pad - False\n",
            "2 conv1_conv - False\n",
            "3 conv1_bn - False\n",
            "4 conv1_relu - False\n",
            "5 pool1_pad - False\n",
            "6 pool1_pool - False\n",
            "7 conv2_block1_1_conv - False\n",
            "8 conv2_block1_1_bn - False\n",
            "9 conv2_block1_1_relu - False\n",
            "10 conv2_block1_2_conv - False\n",
            "11 conv2_block1_2_bn - False\n",
            "12 conv2_block1_2_relu - False\n",
            "13 conv2_block1_0_conv - False\n",
            "14 conv2_block1_3_conv - False\n",
            "15 conv2_block1_0_bn - False\n",
            "16 conv2_block1_3_bn - False\n",
            "17 conv2_block1_add - False\n",
            "18 conv2_block1_out - False\n",
            "19 conv2_block2_1_conv - False\n",
            "20 conv2_block2_1_bn - False\n",
            "21 conv2_block2_1_relu - False\n",
            "22 conv2_block2_2_conv - False\n",
            "23 conv2_block2_2_bn - False\n",
            "24 conv2_block2_2_relu - False\n",
            "25 conv2_block2_3_conv - False\n",
            "26 conv2_block2_3_bn - False\n",
            "27 conv2_block2_add - False\n",
            "28 conv2_block2_out - False\n",
            "29 conv2_block3_1_conv - False\n",
            "30 conv2_block3_1_bn - False\n",
            "31 conv2_block3_1_relu - False\n",
            "32 conv2_block3_2_conv - False\n",
            "33 conv2_block3_2_bn - False\n",
            "34 conv2_block3_2_relu - False\n",
            "35 conv2_block3_3_conv - False\n",
            "36 conv2_block3_3_bn - False\n",
            "37 conv2_block3_add - False\n",
            "38 conv2_block3_out - False\n",
            "39 conv3_block1_1_conv - False\n",
            "40 conv3_block1_1_bn - False\n",
            "41 conv3_block1_1_relu - False\n",
            "42 conv3_block1_2_conv - False\n",
            "43 conv3_block1_2_bn - False\n",
            "44 conv3_block1_2_relu - False\n",
            "45 conv3_block1_0_conv - False\n",
            "46 conv3_block1_3_conv - False\n",
            "47 conv3_block1_0_bn - False\n",
            "48 conv3_block1_3_bn - False\n",
            "49 conv3_block1_add - False\n",
            "50 conv3_block1_out - False\n",
            "51 conv3_block2_1_conv - False\n",
            "52 conv3_block2_1_bn - False\n",
            "53 conv3_block2_1_relu - False\n",
            "54 conv3_block2_2_conv - False\n",
            "55 conv3_block2_2_bn - False\n",
            "56 conv3_block2_2_relu - False\n",
            "57 conv3_block2_3_conv - False\n",
            "58 conv3_block2_3_bn - False\n",
            "59 conv3_block2_add - False\n",
            "60 conv3_block2_out - False\n",
            "61 conv3_block3_1_conv - False\n",
            "62 conv3_block3_1_bn - False\n",
            "63 conv3_block3_1_relu - False\n",
            "64 conv3_block3_2_conv - False\n",
            "65 conv3_block3_2_bn - False\n",
            "66 conv3_block3_2_relu - False\n",
            "67 conv3_block3_3_conv - False\n",
            "68 conv3_block3_3_bn - False\n",
            "69 conv3_block3_add - False\n",
            "70 conv3_block3_out - False\n",
            "71 conv3_block4_1_conv - False\n",
            "72 conv3_block4_1_bn - False\n",
            "73 conv3_block4_1_relu - False\n",
            "74 conv3_block4_2_conv - False\n",
            "75 conv3_block4_2_bn - False\n",
            "76 conv3_block4_2_relu - False\n",
            "77 conv3_block4_3_conv - False\n",
            "78 conv3_block4_3_bn - False\n",
            "79 conv3_block4_add - False\n",
            "80 conv3_block4_out - False\n",
            "81 conv4_block1_1_conv - True\n",
            "82 conv4_block1_1_bn - True\n",
            "83 conv4_block1_1_relu - True\n",
            "84 conv4_block1_2_conv - True\n",
            "85 conv4_block1_2_bn - True\n",
            "86 conv4_block1_2_relu - True\n",
            "87 conv4_block1_0_conv - True\n",
            "88 conv4_block1_3_conv - True\n",
            "89 conv4_block1_0_bn - True\n",
            "90 conv4_block1_3_bn - True\n",
            "91 conv4_block1_add - True\n",
            "92 conv4_block1_out - True\n",
            "93 conv4_block2_1_conv - True\n",
            "94 conv4_block2_1_bn - True\n",
            "95 conv4_block2_1_relu - True\n",
            "96 conv4_block2_2_conv - True\n",
            "97 conv4_block2_2_bn - True\n",
            "98 conv4_block2_2_relu - True\n",
            "99 conv4_block2_3_conv - True\n",
            "100 conv4_block2_3_bn - True\n",
            "101 conv4_block2_add - True\n",
            "102 conv4_block2_out - True\n",
            "103 conv4_block3_1_conv - True\n",
            "104 conv4_block3_1_bn - True\n",
            "105 conv4_block3_1_relu - True\n",
            "106 conv4_block3_2_conv - True\n",
            "107 conv4_block3_2_bn - True\n",
            "108 conv4_block3_2_relu - True\n",
            "109 conv4_block3_3_conv - True\n",
            "110 conv4_block3_3_bn - True\n",
            "111 conv4_block3_add - True\n",
            "112 conv4_block3_out - True\n",
            "113 conv4_block4_1_conv - True\n",
            "114 conv4_block4_1_bn - True\n",
            "115 conv4_block4_1_relu - True\n",
            "116 conv4_block4_2_conv - True\n",
            "117 conv4_block4_2_bn - True\n",
            "118 conv4_block4_2_relu - True\n",
            "119 conv4_block4_3_conv - True\n",
            "120 conv4_block4_3_bn - True\n",
            "121 conv4_block4_add - True\n",
            "122 conv4_block4_out - True\n",
            "123 conv4_block5_1_conv - True\n",
            "124 conv4_block5_1_bn - True\n",
            "125 conv4_block5_1_relu - True\n",
            "126 conv4_block5_2_conv - True\n",
            "127 conv4_block5_2_bn - True\n",
            "128 conv4_block5_2_relu - True\n",
            "129 conv4_block5_3_conv - True\n",
            "130 conv4_block5_3_bn - True\n",
            "131 conv4_block5_add - True\n",
            "132 conv4_block5_out - True\n",
            "133 conv4_block6_1_conv - True\n",
            "134 conv4_block6_1_bn - True\n",
            "135 conv4_block6_1_relu - True\n",
            "136 conv4_block6_2_conv - True\n",
            "137 conv4_block6_2_bn - True\n",
            "138 conv4_block6_2_relu - True\n",
            "139 conv4_block6_3_conv - True\n",
            "140 conv4_block6_3_bn - True\n",
            "141 conv4_block6_add - True\n",
            "142 conv4_block6_out - True\n",
            "143 conv5_block1_1_conv - True\n",
            "144 conv5_block1_1_bn - True\n",
            "145 conv5_block1_1_relu - True\n",
            "146 conv5_block1_2_conv - True\n",
            "147 conv5_block1_2_bn - True\n",
            "148 conv5_block1_2_relu - True\n",
            "149 conv5_block1_0_conv - True\n",
            "150 conv5_block1_3_conv - True\n",
            "151 conv5_block1_0_bn - True\n",
            "152 conv5_block1_3_bn - True\n",
            "153 conv5_block1_add - True\n",
            "154 conv5_block1_out - True\n",
            "155 conv5_block2_1_conv - True\n",
            "156 conv5_block2_1_bn - True\n",
            "157 conv5_block2_1_relu - True\n",
            "158 conv5_block2_2_conv - True\n",
            "159 conv5_block2_2_bn - True\n",
            "160 conv5_block2_2_relu - True\n",
            "161 conv5_block2_3_conv - True\n",
            "162 conv5_block2_3_bn - True\n",
            "163 conv5_block2_add - True\n",
            "164 conv5_block2_out - True\n",
            "165 conv5_block3_1_conv - True\n",
            "166 conv5_block3_1_bn - True\n",
            "167 conv5_block3_1_relu - True\n",
            "168 conv5_block3_2_conv - True\n",
            "169 conv5_block3_2_bn - True\n",
            "170 conv5_block3_2_relu - True\n",
            "171 conv5_block3_3_conv - True\n",
            "172 conv5_block3_3_bn - True\n",
            "173 conv5_block3_add - True\n",
            "174 conv5_block3_out - True\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 100352)            0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 100352)           401408    \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               25690368  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 102)               6630      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,729,062\n",
            "Trainable params: 48,024,358\n",
            "Non-trainable params: 1,704,704\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 100352)            0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 100352)           401408    \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               25690368  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 102)               6630      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,729,062\n",
            "Trainable params: 48,024,358\n",
            "Non-trainable params: 1,704,704\n",
            "_________________________________________________________________\n",
            "None\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - ETA: 0s - loss: 4.4459 - accuracy: 0.0529 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 500s 16s/step - loss: 4.4459 - accuracy: 0.0529 - val_loss: 119.2031 - val_accuracy: 0.0088\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 492s 16s/step - loss: 3.5504 - accuracy: 0.2275 - val_loss: 921.5319 - val_accuracy: 0.0088\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 494s 16s/step - loss: 2.8431 - accuracy: 0.4480 - val_loss: 25.1181 - val_accuracy: 0.0392\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 497s 16s/step - loss: 2.2971 - accuracy: 0.6010 - val_loss: 998.2975 - val_accuracy: 0.0167\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 495s 16s/step - loss: 1.7825 - accuracy: 0.7265 - val_loss: 455.3291 - val_accuracy: 0.0137\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 502s 16s/step - loss: 1.4487 - accuracy: 0.7745 - val_loss: 7.6273 - val_accuracy: 0.2225\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 1.2344 - accuracy: 0.8118 - val_loss: 34.7173 - val_accuracy: 0.1853\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 497s 16s/step - loss: 1.0786 - accuracy: 0.8333 - val_loss: 27.3551 - val_accuracy: 0.1549\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 496s 16s/step - loss: 0.9161 - accuracy: 0.8539 - val_loss: 16902.1719 - val_accuracy: 0.0108\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 496s 16s/step - loss: 0.7812 - accuracy: 0.8755 - val_loss: 4.6871 - val_accuracy: 0.2549\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 490s 15s/step - loss: 0.6142 - accuracy: 0.9020 - val_loss: 40.6497 - val_accuracy: 0.0637\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 491s 15s/step - loss: 0.4925 - accuracy: 0.9235 - val_loss: 3.8971 - val_accuracy: 0.3833\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 491s 15s/step - loss: 0.3408 - accuracy: 0.9510 - val_loss: 2.7783 - val_accuracy: 0.4833\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 492s 15s/step - loss: 0.2689 - accuracy: 0.9657 - val_loss: 1.5264 - val_accuracy: 0.6431\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 491s 15s/step - loss: 0.1798 - accuracy: 0.9784 - val_loss: 1.0588 - val_accuracy: 0.7520\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 491s 15s/step - loss: 0.1391 - accuracy: 0.9912 - val_loss: 1.0236 - val_accuracy: 0.7608\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 489s 15s/step - loss: 0.1090 - accuracy: 0.9922 - val_loss: 1.0375 - val_accuracy: 0.7510\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 492s 16s/step - loss: 0.0833 - accuracy: 0.9961 - val_loss: 0.9007 - val_accuracy: 0.7814\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 490s 15s/step - loss: 0.0935 - accuracy: 0.9922 - val_loss: 0.9444 - val_accuracy: 0.7667\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 493s 16s/step - loss: 0.0770 - accuracy: 0.9931 - val_loss: 0.9336 - val_accuracy: 0.7853\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 491s 15s/step - loss: 0.0944 - accuracy: 0.9912 - val_loss: 2.7558 - val_accuracy: 0.5157\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 492s 16s/step - loss: 0.0901 - accuracy: 0.9941 - val_loss: 0.9431 - val_accuracy: 0.7696\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 495s 16s/step - loss: 0.0780 - accuracy: 0.9922 - val_loss: 0.9732 - val_accuracy: 0.7569\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 497s 16s/step - loss: 0.0783 - accuracy: 0.9931 - val_loss: 0.9858 - val_accuracy: 0.7539\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 0.0688 - accuracy: 0.9922 - val_loss: 0.9427 - val_accuracy: 0.7618\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 496s 16s/step - loss: 0.0811 - accuracy: 0.9892 - val_loss: 1.1433 - val_accuracy: 0.7255\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 0.0974 - accuracy: 0.9873 - val_loss: 1.3048 - val_accuracy: 0.6716\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 496s 16s/step - loss: 0.0900 - accuracy: 0.9853 - val_loss: 1.2113 - val_accuracy: 0.7167\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 494s 16s/step - loss: 0.0800 - accuracy: 0.9873 - val_loss: 1.2085 - val_accuracy: 0.7020\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 0.0769 - accuracy: 0.9892 - val_loss: 1.3351 - val_accuracy: 0.6775\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 0.0977 - accuracy: 0.9843 - val_loss: 1.4244 - val_accuracy: 0.6696\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 0.1097 - accuracy: 0.9804 - val_loss: 1.8635 - val_accuracy: 0.6020\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 498s 16s/step - loss: 0.1002 - accuracy: 0.9824 - val_loss: 1.3368 - val_accuracy: 0.6882\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 505s 16s/step - loss: 0.1202 - accuracy: 0.9735 - val_loss: 1.4837 - val_accuracy: 0.6735\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 499s 16s/step - loss: 0.1240 - accuracy: 0.9755 - val_loss: 1.4229 - val_accuracy: 0.6725\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 499s 16s/step - loss: 0.1568 - accuracy: 0.9647 - val_loss: 2.2957 - val_accuracy: 0.5608\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 496s 16s/step - loss: 0.2178 - accuracy: 0.9461 - val_loss: 2.1138 - val_accuracy: 0.5657\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 500s 16s/step - loss: 0.2147 - accuracy: 0.9451 - val_loss: 2.9029 - val_accuracy: 0.4725\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 494s 16s/step - loss: 0.2406 - accuracy: 0.9500 - val_loss: 1.8161 - val_accuracy: 0.6078\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 495s 16s/step - loss: 0.1975 - accuracy: 0.9500 - val_loss: 2.2326 - val_accuracy: 0.5108\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 493s 16s/step - loss: 0.1713 - accuracy: 0.9608 - val_loss: 1.5092 - val_accuracy: 0.6422\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 496s 16s/step - loss: 0.1527 - accuracy: 0.9618 - val_loss: 1.8659 - val_accuracy: 0.6176\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 497s 16s/step - loss: 0.1340 - accuracy: 0.9686 - val_loss: 1.3982 - val_accuracy: 0.6676\n",
            "Epoch 44/60\n",
            " 8/32 [======>.......................] - ETA: 4:28 - loss: 0.1016 - accuracy: 0.9722"
          ]
        }
      ],
      "source": [
        "# configuration\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import math\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sn; sn.set(font_scale=1.4)\n",
        "from sklearn.utils import shuffle           \n",
        "import matplotlib.pyplot as plt             \n",
        "import cv2                                 \n",
        "import tensorflow as tf                \n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 32\n",
        "optimizer = K.optimizers.Adam()\n",
        "num_classes = len(np.unique(test_labels))\n",
        "freez_layers = 81\n",
        "\n",
        "# creating the CNN + Resnet50 model\n",
        "model = create_model(freez_layers)\n",
        "model.summary()\n",
        "\n",
        "# training phase\n",
        "history = train(model, epochs=60, data_augmentation= True , opt=optimizer,)\n",
        "saved_model = load_model('best_model.h5')\n",
        "#scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('training accuracy')\n",
        "plt.ylabel('training accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Conv'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('validation accuracy')\n",
        "plt.ylabel('validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Conv'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fellow_ai_challenge.ipynb",
      "provenance": [],
      "mount_file_id": "1JMhLHXSc1C8TDtKz7UpN4tXXTrXBfPKG",
      "authorship_tag": "ABX9TyNrK+Q32mTb5qpqh9oaQBtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}